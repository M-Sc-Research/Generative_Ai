
> In large language models (LLMs), data parallelism and model parallelism are often used together to train the model efficiently. Data parallelism involves splitting the training data across multiple devices and updating the model parameters in parallel, while model parallelism involves splitting the model itself across multiple devices and performing computations in parallel. By combining these two parallelization techniques, it is possible to scale training to very large models and datasets.

> Scaling laws for pre-training large language models consider
  - Model size: Number of parameters
  - Dataset size: Number of tokens
  - Batch size: Number of samples per iteration
  - Compute budget: Compute constraints
  <br>
 to maximize the performance of a model within a set of constraints and available scaling choices.
<br>

  > Increasing the model size is one way to potentially improve performance, but it is not the only factor that affects model performance. Other factors such as dataset size, quality of data, training duration, optimization techniques, and hyperparameter tuning also play a crucial role in improving model performance. In some cases, increasing the model size may not necessarily lead to better performance and can even introduce issues like overfitting. It is important to carefully consider all aspects of model training and optimization to achieve the best performance, rather than solely relying on increasing the model size.

<br>

  > The sequence-to-sequence (seq2seq) model architecture is well-suited for text translation tasks. It consists of an encoder-decoder framework that can take in an input sequence (source language) and generate an output sequence (target language) in a different language. This architecture is commonly used in transformer-based models for tasks like machine translation.

<br>


> The transformer-based model architecture that has the objective of guessing a masked token based on the previous sequence of tokens by building bidirectional representations of the input sequence is the Autoencoder. In this architecture, the model is trained to reconstruct the input sequence from a corrupted or masked version of the input, which involves predicting the missing or masked tokens based on the context provided by the rest of the sequence.

<br>

> A mechanism that allows a model to focus on different parts of the input sequence during computation.
  Self-attention is a mechanism in the transformer architecture that allows the model to focus on different parts of the input sequence during computation. It enables the model to weigh the importance of different input tokens when processing a particular token, capturing dependencies and relationships between tokens in the sequence.

> The task that supports the use case of converting code comments into executable code is "Invoke actions from text." This task involves interpreting natural language text (such as code comments) and translating it into executable code or actions.

<br><br>

> Working with Large Language Models (LLMs) involves providing natural language input, known as a "prompt," and receiving output from the model, known as a "completion." The prompt guides the model on what task to perform or what information to generate, while the completion is the response generated by the model based on the input prompt.
