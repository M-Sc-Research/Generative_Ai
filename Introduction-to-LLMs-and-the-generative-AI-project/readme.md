Generative AI is a subset of traditional machine learning. 
<br>
The machine learning models that underpin generative AI have learned these abilities by finding statistical patterns in massive datasets of content that was originally generated by humans.
<br>
Large language models have been trained on trillions of words over many weeks and months and with large amounts of computing power. These foundation models, as we call them, with billions of parameters, exhibit emergent properties beyond language alone, and researchers are unlocking their ability to break down complex tasks, reason, and problem-solve.
<br>
The more parameters a model has, the more memory, and as it turns out, the more sophisticated the tasks it can perform. By either using these models as they are or by applying fine-tuning techniques to adapt them to your specific use case, you can rapidly build customized solutions without the need to train a new model from scratch.<br>
The way you interact with language models is quite different than other machine learning and programming paradigms. In those cases, you write computer code with formalized syntax to interact with libraries and APIs.
<br>

> In contrast, large language models can take natural language or human written instructions and perform tasks much as a human would. The text that you pass to an LLM is known as a **prompt**.

<br>
The space or memory that is available to the prompt is called the context window, and this is typically large enough for a few thousand words but differs from model to model.
<br>

In this example, you ask the model to determine where Ganymede is located in the solar system. The prompt is passed to the model, the model then predicts the next words, and because your prompt contained a question, this model generates an answer. The output of the model is called a **completion**, and the act of using the model to generate text is known as **inference**.

<br>
<br>

> The completion is comprised of the text contained in the original prompt, followed by the generated text. You can see that this model did a good job of answering your question. It correctly **identifies** that Ganymede is a moon of Jupiter and generates a reasonable answer to your question stating that the moon is located within Jupiter's orbit.
<br>

 You can use LLMs to carry out smaller, focused tasks like **information retrieval**. In this example, you ask the model to identify all of the people and places identified in a news article. This is known as named entity recognition, a word classification.

 <br>
 <br>
 
  The understanding of knowledge encoded in the model's parameters allows it:
  1. to correctly carry out this task
  2. return the requested information
  3.  Finally, an area of active development is augmenting LLMs by connecting them to external data sources or using them to invoke external APIs.
  4.   You can use this ability to provide the model with information it doesn't know from its pre-training and to enable your model to power interactions with the real world.
  <br>
  
 > Developers have discovered that as the scale of foundation models grows from hundreds of millions of parameters to billions, even hundreds of billions, the subjective understanding of language that a model possesses also increases. This language understanding stored within the parameters of the model is what processes, reasons, and ultimately solves the tasks you give it, but it's also true that smaller models can be fine-tuned to perform well on specific focused tasks.

 <br>
 
 <p>
	 
It's important to note that generative algorithms are not new. Previous generations of language models made use of an architecture called **recurrent neural networks** or RNNs. RNNs while powerful for their time, were limited by the amount of compute and memory needed to perform well at generative tasks.
</p>

example of an RNN:
- carrying out a simple next-word prediction generative task
-  With just one previous word seen by the model, the prediction can't be very good.
-   As you scale the RNN implementation to be able to see more of the preceding words in the text, you have to significantly scale the resources that the model uses.
<br>

- As for the prediction, the model failed here. Even though you scale the model, it still hasn't seen enough of the input to make a good prediction.
-  To successfully predict the next word, models need to see more than just the previous few words.
- Models need to have an understanding of the whole sentence or even the whole document.
-  The problem here is that language is complex.
-   In many languages, one word can have multiple meanings. These are homonyms.

> Transformers:

- can be scaled efficiently to use multi-core GPUs
-  it can parallel process input data
-   making use of much larger training datasets
-    it's able to learn to pay attention to the meaning of the words it's processing. And attention is all you need. 


<br>

1. The Transformer model uses self-attention to compute representations of input sequences, which allows it to capture long-term dependencies and parallelize computation effectively.
2.  The authors demonstrate that their model achieves state-of-the-art performance on several machine translation tasks and outperforms previous models that rely on RNNs or CNNs.

			


<br>







 > The attention map:
 can be defined as the weights assigned to each input feature when computing the context vector. ex:  can be useful to illustrate the attention weights between each word and every other word.


>  Self-attention: is a mechanism that allows the model to capture relationships between different positions of the input sequence. Self-attention is particularly useful in capturing long-range dependencies and identifying important elements within the sequence.


 self-attention and the ability to learn a tension in this way across the whole input significantly approve the model's ability to encode language. 

![Screenshot from 2024-03-08 01-51-14](https://github.com/M-Sc-Research/Generative_Ai/assets/96652895/5d882aeb-8194-4e92-9fe3-8c7e7b275da8)


The transformer architecture is split into two distinct parts:
1. the encoder
2.  the decoder
 These components work in conjunction with each other and they share a number of similarities.

 ![Screenshot from 2024-03-08 01-40-26](https://github.com/M-Sc-Research/Generative_Ai/assets/96652895/c9878c6e-eb32-41fe-8f34-0d7f6e0e824f)

 
 1. tokenize the words. Simply put, this converts the words into numbers
 2. with each number representing a position in a dictionary of all the possible words that the model can work with
 3. WE can choose from multiple tokenization methods. For example, token IDs matching two complete words, or using token IDs to represent parts of words.
 4.   What's important is that once you've selected a tokenizer to train the model, you must use the same tokenizer when you generate text.
 5.   Now that your input is represented as numbers, you can pass it to the embedding layer. This layer is a trainable vector embedding space, a high-dimensional space where each token is represented as a vector and occupies a unique location within that space.
 6.    Each token ID in the vocabulary is matched to a multi-dimensional vector, and the intuition is that these vectors learn to encode the meaning and context of individual tokens in the input sequence.
 7. Embedding vector spaces have been used in natural language processing for some time, previous generation language algorithms like Word2vec use this concept.
 8. if you imagine a vector size of just three, you could plot the words into a three-dimensional space and see the relationships between those words.
 9. As you add the token vectors into the base of the encoder or the decoder, you also add positional encoding.
 10. The model processes each of the input tokens in parallel.
 11.  Once you've summed the input tokens and the positional encodings, you pass the resulting vectors to the self-attention layer.
 12.   Here, the model analyzes the relationships between the tokens in your input sequence. As you saw earlier, this allows the model to attend to different parts of the input sequence to better capture the contextual dependencies between the words.
 13.   The self-attention weights that are learned during training and stored in these layers reflect the importance of each word in that input sequence to all other words in the sequence. But this does not happen just once, the transformer architecture actually has multi-headed self-attention.
 14.    This means that multiple sets of self-attention weights or heads are learned in parallel independently of each other.
 15. The number of attention heads included in the attention layer varies from model to model, but numbers in the range of 12-100 are common.
 16.  The intuition here is that each self-attention head will learn a different aspect of language.
 17.  The weights of each head are randomly initialized and given sufficient training data and time, each will learn different aspects of language.
 18.    Now that all of the attention weights have been applied to your input data, the output is processed through a fully connected feed-forward network.
 19. The output of this layer is a vector of logits proportional to the probability score for each and every token in the tokenizer dictionary. You can then pass these logits to a final softmax layer, where they are normalized into a probability score for each word.
 20.  This output includes a probability for every single word in the vocabulary, so there are likely to be thousands of scores here.
 21.   One single token will have a score higher than the rest. This is the most likely predicted token.


<br>

example: translate

![Screenshot from 2024-03-08 02-30-07](https://github.com/M-Sc-Research/Generative_Ai/assets/96652895/fb8fa72e-01da-4dd9-bf6f-45e49bf525eb)

## Transformer:
![Screenshot from 2024-03-08 02-32-54](https://github.com/M-Sc-Research/Generative_Ai/assets/96652895/8e6ed441-a729-4c5b-bb53-ed75a9242751)

<br>

![Screenshot from 2024-03-08 02-35-09](https://github.com/M-Sc-Research/Generative_Ai/assets/96652895/956e1aae-5893-4648-88bc-3937de3229ed)

<br>

The Transformer model uses self-attention to compute representations of input sequences, which allows it to capture long-term dependencies and parallelize computation effectively.
<br>

<strong>
The Transformer architecture consists of an encoder and a decoder, each of which is composed of several layers. Each layer consists of two sub-layers: a multi-head self-attention mechanism and a feed-forward neural network. The multi-head self-attention mechanism allows the model to attend to different parts of the input sequence, while the feed-forward network applies a point-wise fully connected layer to each position separately and identically. 
The Transformer model also uses residual connections and layer normalization to facilitate training and prevent overfitting.
</strong>


<br>

## Prompting and prompt engineering

**The text that you feed into the model is called the prompt, the act of generating text is known as inference, and the output text is known as the completion.**

#### context window:
The full amount of text or the memory that is available to use for the prompt is called the context window.
<br>
#### prompt engineering:
 you'll frequently encounter situations where the model doesn't produce the outcome that you want on the first try. <br>
 You may have to revise the language in your prompt or the way that it's written several times to get the model to behave in the way that you want. <br>
 This work to develop and improve the prompt is known as prompt engineering.

 <br>
 This is a big topic. But one powerful strategy to get the model to produce better outcomes is to include examples of the task that you want the model to carry out inside the prompt. Providing examples inside the context window is called in-context learning. Let's take a look at what this term means. With in-context learning, you can help LLMs learn more about the task being asked by including examples or additional data in the prompt. Here is a concrete example. Within the prompt shown here, you ask the model to classify the sentiment of a review. So whether the review of this movie is positive or negative, the prompt consists of the instruction, "Classify this review," followed by some context, which in this case is the review text itself, and an instruction to produce the sentiment at the end. This method, including your input data within the prompt, is called zero-shot inference. The largest of the LLMs are surprisingly good at this, grasping the task to be completed and returning a good answer. In this example, the model correctly identifies the sentiment as positive. Smaller models, on the other hand, can struggle with this. Here's an example of a completion generated by GPT-2, an earlier smaller version of the model that powers ChatGPT. As you can see, the model doesn't follow the instruction. While it does generate text with some relation to the prompt, the model can't figure out the details of the task and does not identify the sentiment. This is where providing an example within the prompt can improve performance. Here you can see that the prompt text is longer and now starts with a completed example that demonstrates the tasks to be carried out to the model. After specifying that the model should classify the review, the prompt text includes a sample review. I loved this movie, followed by a completed sentiment analysis. In this case, the review is positive. Next, the prompt states the instruction again and includes the actual input review that we want the model to analyze. You pass this new longer prompt to the smaller model, which now has a better chance of understanding the task you're specifying and the format of the response that you want. The inclusion of a single example is known as one-shot inference, in contrast to the zero-shot prompt you supplied earlier. Sometimes a single example won't be enough for the model to learn what you want it to do. So you can extend the idea of giving a single example to include multiple examples. This is known as few-shot inference. Here, you're working with an even smaller model that failed to carry out good sentiment analysis with one-shot inference. Instead, you're going to try few-shot inference by including a second example. This time, a negative review, including a mix of examples with different output classes can help the model to understand what it needs to do. You pass the new prompts to the model. And this time it understands the instruction and generates a completion that correctly identifies the sentiment of the review as negative. So to recap, you can engineer your prompts to encourage the model to learn by examples. While the largest models are good at zero-shot inference with no examples, smaller models can benefit from one-shot or few-shot inference that include examples of the desired behavior. But remember the context window because you have a limit on the amount of in-context learning that you can pass into the model. Generally, if you find that your model isn't performing well when, say, including five or six examples, you should try fine-tuning your model instead. Fine-tuning performs additional training on the model using new data to make it more capable of the task you want it to perform. You'll explore fine-tuning in detail in week 2 of this course. As larger and larger models have been trained, it's become clear that the ability of models to perform multiple tasks and how well they perform those tasks depends strongly on the scale of the model. As you heard earlier in the lesson, models with more parameters are able to capture more understanding of language. The largest models are surprisingly good at zero-shot inference and are able to infer and successfully complete many tasks that they were not specifically trained to perform. In contrast, smaller models are generally only good at a small number of tasks. Typically, those that are similar to the task that they were trained on. You may have to try out a few models to find the right one for your use case. Once you've found the model that is working for you, there are a few settings that you can experiment with to influence the structure and style of the completions that the model generates.
